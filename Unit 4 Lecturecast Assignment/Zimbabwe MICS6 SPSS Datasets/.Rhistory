library(haven)
mn <- read_sav("mn.sav")
View(mn)
# Install and load the haven package (if not already installed)
if (!requireNamespace("haven", quietly = TRUE)) {
install.packages("haven")
}
library(haven)
# Specify the file path of the .sav file
input_file <- "mn.sav"
# Import the .sav file into R
data <- read_sav(C:\Users\NeOjA\Downloads\Zimbabwe MICS6 SPSS Datasets\mn.sav)
> library(haven)
input_file <- "C:/Users/NeOjA/Downloads/Zimbabwe MICS6 SPSS Datasets/mn.sav"
# Import the .sav file into R
data <- read_sav(input_file)
# Specify the output CSV file path
output_file <- "C:/Users/NeOjA/Downloads/Zimbabwe MICS6 SPSS Datasets/mn.csv"
# Save the data as a CSV file
write.csv(data, output_file, row.names = FALSE)
# Print confirmation
cat("File converted to CSV and saved as", output_file, "\n")
# URL of the World Bank Microdata page
url <- "https://microdata.worldbank.org/index.php/catalog/4180/data-dictionary/F7?file_name=mn.sav"
# Read the HTML content of the page
webpage <- read_html(url)
# Install and load required packages
if (!requireNamespace("rvest", quietly = TRUE)) {
install.packages("rvest")
}
if (!requireNamespace("tidyverse", quietly = TRUE)) {
install.packages("tidyverse")
}
library(rvest)
library(tidyverse)
# URL of the World Bank Microdata page
url <- "https://microdata.worldbank.org/index.php/catalog/4180/data-dictionary/F7?file_name=mn.sav"
# Read the HTML content of the page
webpage <- read_html(url)
# Extract the table containing the variables
variables_table <- webpage %>%
html_element("table") %>%          # Find the first table on the page
html_table()
# Display the first few rows of the table
print(head(variables_table))
# Save the extracted data to a CSV file
write.csv(variables_table, "mn_variables.csv", row.names = FALSE)
# Confirmation message
cat("Variables and metadata saved to mn_variables.csv\n")
# URL of the webpage to scrape
url <- "https://microdata.worldbank.org/index.php/catalog/4180/data-dictionary/F7?file_name=mn.sav"
# Read the HTML content of the page
webpage <- read_html(url)
# Extract all text content from the body
all_text <- webpage %>%
html_element("body") %>%
html_text2()
# Display the first 500 characters (optional)
cat(substr(all_text, 1, 500), "...\n")
# Save all text to a text file for review
writeLines(all_text, "scraped_content.txt")
# Confirmation message
cat("All text content has been scraped and saved to 'scraped_content.txt'\n")
# URL of the webpage to scrape
url <- "https://microdata.worldbank.org/index.php/catalog/4180/data-dictionary/F7?file_name=mn.sav"
# Read the HTML content of the page
webpage <- read_html(url)
# Extract all text content (visible text)
all_text <- webpage %>%
html_element("body") %>%
html_text2()
# Split the text into lines (based on line breaks)
lines <- str_split(all_text, "\n")[[1]]
# Clean up whitespace and filter empty lines
cleaned_lines <- lines %>%
str_trim() %>%
.[. != ""]
# Create a data frame for better organization
data <- tibble(Line_Number = seq_along(cleaned_lines), Text = cleaned_lines)
# Save the data to a CSV file
write_csv(data, "scraped_content.csv")
# Confirmation message
cat("All content has been scraped and saved to 'scraped_content.csv'\n")
